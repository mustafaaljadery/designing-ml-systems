# 3. Training Data

Despite the importance of training data in developing and improving ML models, ML curricula are heavily skewed towards modeling, which is considered by many researchers and engineers as the "fun" part of the process.

"Training data" is used over "training datasets" becaused datasets mean they are finite and stationery.

Creating training data is an interative process.

Data is full of potential biasess. The biases may have causes.

## Sampling

Sampling a dataset means selecting a subset of data points from a larger dataset. It's done to analyze or work with a manageable portion of the data instead of the entire set.

Sampling is necessary for most applications. Examples: 
1. You don't have access to all possible data in the world. 
2. It's infeasible to process all the data that you have to access.

Sampling is useful because it allows you accomplish a task faster and cheaper.

### Non-Probability Sampling
- When the selection of data isn't based on any probability criteria.
- Convenience Sampling: samples of data are selected based on their availablity.
- Snowball sampling: future sampels are selected based on existing samples.
- Judgement sampling: experts decide what samples to include.
- Quota sampling: you select samples based on quotas for certain slices of data without any randomization.

This type of sampling is open to all sorts of biases.

### Simple Random Sampling

You give all samples in the population equal probabilities of being selected.

The advantage of this method is that it's easy to implement.

The drawback is that rare categories of data might not appear in your selection.

### Stratified Sampling

To avoid the drawback fo simple random sampling, you can first divide your population into the groups that you care about and sample from each group separately.

You can sample 1% of data that has two classes A and B. This means you get 1% of classes A and 1% of classes B. This way no matter how rare class A or B is, you'll ensure that samples from it will be included in the selection.

You can't always do this because sometimes it's impossible to divide all samples into groups.

### Weighted Sampling

Each sample is given a weight, which determeins the probability of it being selected.

This method allows you to leverage domain expertise.

This also helps with the case when the dat ayou have comes from a different distribution.

Weighted sampling is used to select samples to train your model with, whereas sample weights are used to assign "weights" or "importance" to training samples.

Samples with higher weights affect the loss function more.

### Importance Sampling

Sample from a distribution when we only have access to another distribution.

This is really important in policy-based reinforcement learning.

### Reservoir Sampling

Especially useful when you have to deal with continually incoming data, which is usually what you have in production.

You keep open spots in your sample as reseviors and then update them as more data comes in.

## Labeling

Most ML models in production today are supervised, which means that they need labels to learn.

The performance of an ML model still depends heavily on the quality and the quantity of the labeled data it's trained on. 

Data labeling has gone from being an auxiliary task to being a core function of many ML teams in production.

### Hand Labels

Hand-labeling data can be expensive, especially if subject matter expertise is required.

Hand labeling poses a threat to data privacy.

Hand labeling is slow.

Slow labeling leads to slow iteration speed.

### Label Multiplicity

You have to use labels from multiple sources and those have different levels of accuracy.

Disagreements among annotators are extremely common. the higher leve of domain expertise required, the higher the potential for annotating disagreement.

## Handling the Lack of Hand Labels

### Weak Supervision
Libraries like Snorkel are built around the concept of labeling function: a funciton that encodes heruistics. 

After you've written LFs, you can apply them to the samples you want to label and they will do them for you. 

In theory, you don't need any hand labels for weak supervision. However, to get a sense of how accurate your LFs are, a small amount of hand labels is recommended.

If heuristics work so well to label data, whyd o we need machine learning models? 
- Labeling functions might not cover all your data samples, so you need to train ML models to generalize to samples that aren't covered in your labeling function. 

Sometimes these labels are too noisy.

## Semi Supervision

Semi-supervision leverages structural assumptions to generate new labels based on a small set of initial labels. You have to have an initial set of labels.

You start by training a modelon yoru existing sert of labeled data and use this mdoel to make predictions for unlabeled samples.

Data samples taht share similar characteristics share the same labels.

Semi supervision is the most useful when the number of raining labels is limited.

## Transfer Learning

The model developed for a task is reused as the starting point for a model on a second task.

Transfer learning is especially appealing for tasks that don't have a lot of labeled data. Even for tasks that have a lot of labeled data, using a pretrained model as the starting point can often boost the performance significantly compared to training from scratch.

## Active Learning

ML models can achieve greater accuracy with fewer training labels if they can choose which data samples to learn from.

Active learning is most exciting when a syste works with real-time data.

You want to very quickly adapt to changing environments.

## Class Imbalance

There is a substancial difference in the number of samples ine ach class of the training data.

ML works well in situations when the data distribution is more balances, and not so well when the classes are heavily imbalanced.

Class imbalances often mens that ther's insufficinet signal.

Class imbalance makes it easier for your model to get stuck in a non-optimal solution.

Class imbalance leads to asymmetric costs of error.

A pretty uncommon cause leading to class imbalance is labeling errors.

Class imbalance affects tasks differently based on the level of imbalance.

The deeper the neural network the better it is at dealing with class imbalances.

### Using the right evaluation metrics

The most importnat thing to do when facing a task with class imbalance is to choose the apppropriate evaluation metrics.

The overall accuracy and error rate are the most frequently used metrics to report the performance of ML models.

Metrics that help you understand your model's performance with respect to specific classes would be better choices.

### Data-level methods: Resampling

Modify the distribution of the training data to reduce the level of imbalance to make it easier for the model to learn.

Resampling includes oversampling, adding more examples from the minority classes and undersampling, removing exampes of the majority classes.

### Algorithm-level methods

Kepp the trainint data distribution intact but alter the algorithm to make it more robust to class imbalance.

Most of algorithm-level methods involve adjustment of the loss function.

The loss function values the loss caused by instances equally, even though wrong predictions ons ome instances might be much costlier than wrong predicitons on other instances.

If classifying positive exampleas as negative is twice as costly as the other way around, you can point this in the loss function.

### Class-balanced loss

You can make the weight of each class inversely proportional to the number fof samples in that class, so that the rarer classes have higher weights.

### Focal Loss

You want to incentivize your model to focus on leraning the samples they still have difficulty classifying.

## Data Augmentation

A family of techniques that are used to increase the amount of training data. 

Data augmentation has become a standard step in many computer vision tasks and is finding its way into natural language processing (NLP) tasks.

### Simple Label-Preserving Transformations

Simplest way to augment is to randomly modify an image while preserving its label.

You can modify the image by cropping, flipping, rotating, inverting (horizontally or verically), erasing part of an image, and more.

In NLP, you can randomly replace a word with a similar word, assuming that this replacement wouldn't change the meaning of the sentiment of the sentence.

### Perturbation

Adding noise to samples to create adversarial samples is a common technique for adversarial attacks.

Adversarial augmentation is less common in NLP, but perturbation has been used to make models more robust.

### Data Synthesis

It's possible to synthesize some training data to boost a model's performance.