# 3. Training Data

Despite the importance of training data in developing and improving ML models, ML curricula are heavily skewed towards modeling, which is considered by many researchers and engineers as the "fun" part of the process.

"Training data" is used over "training datasets" becaused datasets mean they are finite and stationery.

Creating training data is an interative process.

Data is full of potential biasess. The biases may have causes.

## Sampling

Sampling a dataset means selecting a subset of data points from a larger dataset. It's done to analyze or work with a manageable portion of the data instead of the entire set.

Sampling is necessary for most applications. Examples: 
1. You don't have access to all possible data in the world. 
2. It's infeasible to process all the data that you have to access.

Sampling is useful because it allows you accomplish a task faster and cheaper.

### Non-Probability Sampling
- When the selection of data isn't based on any probability criteria.
- Convenience Sampling: samples of data are selected based on their availablity.
- Snowball sampling: future sampels are selected based on existing samples.
- Judgement sampling: experts decide what samples to include.
- Quota sampling: you select samples based on quotas for certain slices of data without any randomization.

This type of sampling is open to all sorts of biases.

### Simple Random Sampling

You give all samples in the population equal probabilities of being selected.

The advantage of this method is that it's easy to implement.

The drawback is that rare categories of data might not appear in your selection.

### Stratified Sampling

To avoid the drawback fo simple random sampling, you can first divide your population into the groups that you care about and sample from each group separately.

You can sample 1% of data that has two classes A and B. This means you get 1% of classes A and 1% of classes B. This way no matter how rare class A or B is, you'll ensure that samples from it will be included in the selection.

You can't always do this because sometimes it's impossible to divide all samples into groups.

### Weighted Sampling

Each sample is given a weight, which determeins the probability of it being selected.

This method allows you to leverage domain expertise.

This also helps with the case when the dat ayou have comes from a different distribution.

Weighted sampling is used to select samples to train your model with, whereas sample weights are used to assign "weights" or "importance" to training samples.

Samples with higher weights affect the loss function more.

### Importance Sampling

Sample from a distribution when we only have access to another distribution.

This is really important in policy-based reinforcement learning.

### Reservoir Sampling

Especially useful when you have to deal with continually incoming data, which is usually what you have in production.

You keep open spots in your sample as reseviors and then update them as more data comes in.

## Labeling

Most ML models in production today are supervised, which means that they need labels to learn.

The performance of an ML model still depends heavily on the quality and the quantity of the labeled data it's trained on. 

Data labeling has gone from being an auxiliary task to being a core function of many ML teams in production.

### Hand Labels

Hand-labeling data can be expensive, especially if subject matter expertise is required.

Hand labeling poses a threat to data privacy.

Hand labeling is slow.

Slow labeling leads to slow iteration speed.

### Label Multiplicity

You have to use labels from multiple sources and those have different levels of accuracy.

Disagreements among annotators are extremely common. the higher leve of domain expertise required, the higher the potential for annotating disagreement.

## Handling the Lack of Hand Labels

### Weak Supervision
Libraries like Snorkel are built around the concept of labeling function: a funciton that encodes heruistics. 

After you've written LFs, you can apply them to the samples you want to label and they will do them for you. 

In theory, you don't need any hand labels for weak supervision. However, to get a sense of how accurate your LFs are, a small amount of hand labels is recommended.

If heuristics work so well to label data, whyd o we need machine learning models? 
- Labeling functions might not cover all your data samples, so you need to train ML models to generalize to samples that aren't covered in your labeling function. 

Sometimes these labels are too noisy.

## Semi Supervision

Semi-supervision leverages structural assumptions to generate new labels based on a small set of initial labels. You have to have an initial set of labels.

You start by training a modelon yoru existing sert of labeled data and use this mdoel to make predictions for unlabeled samples.

Data samples taht share similar characteristics share the same labels.

Semi supervision is the most useful when the number of raining labels is limited.

## Transfer Learning

The model developed for a task is reused as the starting point for a model on a second task.

Transfer learning is especially appealing for tasks that don't have a lot of labeled data. Even for tasks that have a lot of labeled data, using a pretrained model as the starting point can often boost the performance significantly compared to training from scratch.

## Active Learning

ML models can achieve greater accuracy with fewer training labels if they can choose which data samples to learn from.

page 173