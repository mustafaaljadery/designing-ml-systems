# 2. Data Engineering Fundamentals

The rise of machine learning in recent years is tightly coupled with the rise of big data.

Big data systems, even without machine learning, are complex.

Success of an ML system depends largely on the data it was trained on.

Instead of focusing on improving ML algorithms, most companies focus on managing and improving their data.

Mind: inductive biases or intelligent architectural designs.

Data: computation. Since more data tends to require more computation.

You can both pursue intelligent design and leverage large data and computation. However, spending time on one often takes time away from another.

Richard Sutton argues tha  those who chose ot pursue intelligent design over methods that leverage computation will eventually learn a bitter lesson.

General methods that leverage computation are ultimately the most effective, and by a large margin... 

*We don't have better algorithms. We just have more data.*

The debate isn't about whether finite data is necesary, but whether it's sufficient.

If we have infinite data, we can just look up the answer. Thus all data is finite if you are training it into an ML model.

Data doesn't always mean better model. Bad data can actually hurt your model.

## Data Sources

Understanding the sources your data comes from can help you use your data more efficiently.

Here are different sources of user data
- User input data: User input can be texts, images, videos, uploaded files, etc. Usually this type of data requires more heavy-duty checking and processing. users also have little patience.
- System-generated data: this is data from a source in your system. Something like logs. This can include large batch jobs for data processing and model training. It's acceptable to process logs periodically, such as hourly or even daily.
- Internal databases: Generated by various services and enterprise applciations in a company. These databases manage their assets such as inventory, customer relationship, users, and more. 
- Third-party data: This is riddled with privacy concerns. From these kinda of datasets you can infer preferences and more.

## Data Formats
- Once you have data, you might want to store it ("persist it")
- The process of converting a data structure or object state into a format that can be stored or transmitted and reconstructured late is data serialization.

### JSON
- Javascript object notation is everywhere.
- Its key-value pair paradigm is simple but powerful.

### Row-major vs Column-major
- The two distinct paradigms are CSV and Parquet.
- Row major: Consecutive elements in a row are stored next to each other in memory.
- Column major: Consecutive elemnts in a column are stored next to each other.

Row is better for accessing samples while column is better for accessing features.

Column-major formats allow flexible column-based reads, especially if your data is large with thousands, if not millions, of features.

Row-major formats allow faster data writes.

Usually row based is better when you have a lot of writes while column based is better when you have a lot of reads.

Pandas is column-major while numpy is row-major.

### Text vs Binary Format
- Text files are files that are in plain texts, which usually mean they are human-readable.
- Binary files, are files that contain 0s and 1s, and meant to be read or used by programs that know how to interpret the raw bytes.
- Binary files are a lot more compact that text files.

AWS recommends using the Parquet format beccause "the Parquet format is up to 2x faster to unload and consumers up to 6x less storage in Amazon S3, compared to text formats."

## Data Processing and Storage
- Transaction processing and analytical processing, and their uses.
- Do you want to store your data as structured or unstructured.

### Transactional and Analytical Databases
- Systems in production generate data. 
- Transactional databases are designed to process online transactions and satisfy all those requirements. Most of the operations they do will be inserting, deleting, and updating an existing transaction. This means they are more row-major.
- Analytics dbs are more so for logs and analytics.

The bridge between transactional and analytical dbs is getting smaller. You have transactional dbs like cockroachdb that can act on analytical queries and vice versa.

Storage and processing are tighltly couples - how data is stored is also how data is processed.

An interesting paradigm is decoupling storage from compute. It's adopted by many data platforms including Snowflake and more.

The feature is online after it has been deployed to production.

Online processing means data is immediately available for input/output.

Offline means data requires some human intervention to become online.

The speed at which applications respond to user queries has become a competitive advantage. It's becom more and more important to make data available for use as fast as possible.

## ETL: Extract, Transform, Load
- General purpose processing and agggregating data into the shape and format that you want.
- Extract: extracting the data you want from different data sources. You want to validate your data and make sure it's good. 
- Transform: You might wnat to join data from multiple sources and clean it. You might want to standardize the value ranges. You can apply operations such as transposing, deduplicating, sorting, aggregating, deriving new features, more data validating, etc...
- Load: Deciding how and how often to load your transformed data into the target destination, which can be a file, a database, or a data warehouse.

### Structured vs unstructured data
- Structured data is data that follows a predefined data model, also known as data schema.
- The disadvantage of structured data is that you have to commit your data to a predefined schema.
- If your schema changes, you'll have to retrospectively udpate all your data and/or the changes will cause mysterious bugs.
- Because busienss requirements change over time, committing to a predefined data schema 
- Unstructured data is data that doens't adhere to a predefined data schema. It's usually text but can also be numbers, dates, etc.
- A repository for storing unstructure data is called a data lake.

Data lakes are usually used to store raw data before processing. 

Data warehouses are used to store data that have been processed into formats ready to be used.

### ETL to ELT
- Extract -> Transform -> Load
- Extract -> Load -> Transform

The idea of ELT becasue less viable as you get a huge dataset.

## Summary

It's very important for ML researchers to know a lot about managing data.